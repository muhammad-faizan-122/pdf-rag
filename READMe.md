# ğŸ“„ PDF Chat App using RAG (Retrieval-Augmented Generation)

A simple Streamlit-based app that lets you **chat with your PDF files** using a **local open-source LLM** powered by [Ollama](https://ollama.com). It utilizes **Retrieval-Augmented Generation (RAG)** to extract and respond based on the content of your documents â€” all offline.

---
## ğŸ–¥ï¸ GUI
![App Screenshot](demo/gui.png)
## âš™ï¸ Installation & Setup

Follow these steps to set up the app locally:

### 1. Create a Virtual Environment (Recommended)

```bash
python -m venv venv
source venv/bin/activate        # On Windows: venv\Scripts\activate
````

### 2. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 3. Install and Configure Ollama

* Download and install Ollama for your OS: [https://ollama.com/download](https://ollama.com/download)
* Pull the required LLM model:

```bash
ollama pull llama3.2:1b
```

### 4. Run the App

```bash
streamlit run app.py
```

---

## ğŸš€ Features

* âœ… Fully offline LLM-based PDF question answering
* âœ… Uses local LLM (e.g., `llama3.2:1b`) via Ollama
* âœ… Simple and clean Streamlit UI

---

## ğŸ“Œ Notes

* Tested on **CPU-only** systems
* Designed for **offline use** (no internet needed after setup)
* Current model: `llama3.2:1b` â€” a lightweight local model with relatively slower inference

---

## ğŸ§  Future Improvements

The app works well as a prototype but can be improved in the following ways:

* ğŸ’„ **UI Enhancements**: Improve the look and feel of the Streamlit interface
* ğŸ§¹ **Auto Clear Database**: Automatically clean up embeddings or cached files when the app closes
* âš¡ **Embedding Optimization**: Skip re-generating embeddings if they already exist for the uploaded file
* ğŸ“ **Multi-file Support**: Enable chatting across multiple PDFs simultaneously
* ğŸš€ **Model Upgrade**: Use a faster/better LLM (e.g., larger model or one running on GPU)
* ğŸ”’ **Security & File Management**: Add session-based cleanup and user-level file management

---

Feel free to fork, improve, or adapt this project for your own use cases!